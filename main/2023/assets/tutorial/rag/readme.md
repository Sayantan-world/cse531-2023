# Introduction to Zero-shot Retrieval-Augmented Generation (RAG) Using Langchain and ChromaDB

### Instructions:
1. Read the story.txt and run the ipynb file.
2. You will need a HF account and a LM/LLM token.

Welcome to this tutorial on Retrieval-Augmented Generation (RAG), a transformative approach in the field of natural language processing that combines the power of language models with the vast knowledge stored in external databases. In this tutorial, we will delve into the fundamentals of RAG, utilizing Langchain and ChromaDB, that facilitate seamless integration of external knowledge sources into language models.

### What is RAG?
Retrieval-Augmented Generation is a technique that enhances the capabilities of language models by allowing them to retrieve and utilize information from external databases. This method not only expands the model's knowledge base beyond its training data but also enables the generation of more accurate, relevant, and contextually rich responses.

### Langchain
Langchain, a versatile library, plays a crucial role in our RAG implementation. It provides an intuitive framework for integrating language models with various external data sources. 

### ChromaDB
ChromaDB stands at the core of our retrieval system. It is a comprehensive, structured database that serves as the external knowledge repository for our RAG model. 

### Practical Application: QnA on a Story "Slay the Spire"
To demonstrate the capabilities of RAG using Langchain and ChromaDB, we will embark on a unique project: creating a QnA model on a short story inspired by the game "Slay the Spire."

### Sources
Some other sources to read more on how to implement RAG:
1. https://haystack.deepset.ai/tutorials/07_rag_generator
2. https://haystack.deepset.ai/tutorials/22_pipeline_with_promptnode
3. https://python.langchain.com/docs/integrations/vectorstores/chroma
4. https://python.langchain.com/docs/expression_language/cookbook/retrieval
5. https://python.langchain.com/docs/expression_language/cookbook/prompt_llm_parser

